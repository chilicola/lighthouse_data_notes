{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From `nltk` we can download translated sentences between different languages. You can see the example between **English and French** below but feel free to try different combination as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package comtrans to\n",
      "[nltk_data]     C:\\Users\\Capt.Otaku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package comtrans is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('comtrans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AlignedSent: 'Resumption of the se...' -> 'Reprise de la sessio...'>\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import comtrans\n",
    "print(comtrans.aligned_sents('alignment-en-fr.txt')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33334"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comtrans.aligned_sents('alignment-en-fr.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22 // 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build\n",
    "[Guide 1](https://hub.packtpub.com/create-an-rnn-based-python-machine-translation-system-tutorial/), \n",
    "[Guide 2](https://towardsdatascience.com/language-translation-with-rnns-d84d43b40571),\n",
    "[Guide from mentor](https://www.analyticsvidhya.com/blog/2019/01/neural-machine-translation-keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import comtrans\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define extract function\n",
    "def extract_corpus(translated_pairs='alignment-en-fr.txt'):\n",
    "    als = comtrans.aligned_sents(translated_pairs)\n",
    "    sent1 = [sent.words for sent in als]\n",
    "    sent2 = [sent.mots for sent in als]\n",
    "    return sent1, sent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract and save corpus\n",
    "source, target = extract_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lowercase\n",
    "def to_lower(sentence):\n",
    "    lower = [word.lower() for word in sentence]\n",
    "    return lower\n",
    "\n",
    "# remove punctuations\n",
    "def remove_punc(sentence):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [word.translate(table) for word in sentence]\n",
    "    return stripped\n",
    "\n",
    "# remove empty strings\n",
    "def remove_empty(sentence):\n",
    "    full = [word for word in sentence if len(word) >= 1]\n",
    "    return full\n",
    "\n",
    "# combine all cleaning methods above\n",
    "def preprocess(sentence):\n",
    "    prep = to_lower(sentence)\n",
    "    prep = remove_punc(prep)\n",
    "    prep = remove_empty(prep)\n",
    "    return prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "clean_source = []\n",
    "clean_target = []\n",
    "\n",
    "for sent in source:\n",
    "    clean_source.append(preprocess(sent))\n",
    "for sent in target:\n",
    "    clean_target.append(preprocess(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract desire length indices\n",
    "def keep_index(sentences, maxlen=20):\n",
    "    keep = []\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        if len(sentence) <= 20:\n",
    "            keep.append(index)\n",
    "    return keep\n",
    "\n",
    "# filter out long sentences to lower computation level\n",
    "def cutoff(sentences, indices):\n",
    "    cut = []\n",
    "    for index in indices:\n",
    "        cut.append(sentences[index])\n",
    "    return cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# limit sentence length to 20 words\n",
    "keep_id = keep_index(clean_source)\n",
    "cut_source = cutoff(clean_source, keep_id)\n",
    "cut_target = cutoff(clean_target, keep_id)\n",
    "\n",
    "keep_id = keep_index(cut_target)\n",
    "cut_source = cutoff(cut_source, keep_id)\n",
    "cut_target = cutoff(cut_target, keep_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    return tokenizer.texts_to_sequences(sentences), tokenizer\n",
    "\n",
    "def padding(source):\n",
    "    return preprocessing.sequence.pad_sequences(source) # need maxlen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform words to unique numeric ID\n",
    "source_tokenized, source_tokenizer = tokenize(cut_source)\n",
    "target_tokenized, target_tokenizer = tokenize(cut_target)\n",
    "\n",
    "# add padding\n",
    "pad_source = padding(source_tokenized)\n",
    "pad_target = padding(target_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13988"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape for LSTM input\n",
    "input_tokens = pad_source.reshape(*pad_source.shape, 1)\n",
    "output_tokens = pad_target.reshape(*pad_target.shape, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16834, 20, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# model.add(Embedding(input_dim=len(target_tokenizer.word_index), output_dim=20))\n",
    "model.add(LSTM(256, return_sequences=True, input_shape=input_tokens.shape[1:]))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 20, 256)           264192    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 20, 128)           32896     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20, 1)             129       \n",
      "=================================================================\n",
      "Total params: 297,217\n",
      "Trainable params: 297,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "14/14 [==============================] - 6s 421ms/step - loss: 3275528.7500 - accuracy: 0.0266 - val_loss: 4651691.0000 - val_accuracy: 0.0263\n",
      "Epoch 2/10\n",
      "14/14 [==============================] - 6s 406ms/step - loss: 3275528.7500 - accuracy: 0.0266 - val_loss: 4651691.0000 - val_accuracy: 0.0263\n",
      "Epoch 3/10\n",
      "14/14 [==============================] - 6s 398ms/step - loss: 3275528.7500 - accuracy: 0.0266 - val_loss: 4651691.0000 - val_accuracy: 0.0263\n",
      "Epoch 4/10\n",
      "14/14 [==============================] - 6s 394ms/step - loss: 3275528.2500 - accuracy: 0.0266 - val_loss: 4651691.0000 - val_accuracy: 0.0263\n",
      "Epoch 5/10\n",
      "14/14 [==============================] - 6s 394ms/step - loss: 3275528.2500 - accuracy: 0.0266 - val_loss: 4651691.0000 - val_accuracy: 0.0263\n",
      "Epoch 6/10\n",
      "14/14 [==============================] - 6s 406ms/step - loss: 3275529.0000 - accuracy: 0.0266 - val_loss: 4651691.0000 - val_accuracy: 0.0263\n",
      "Epoch 7/10\n",
      "14/14 [==============================] - 6s 396ms/step - loss: 3275528.7500 - accuracy: 0.0266 - val_loss: 4651691.0000 - val_accuracy: 0.0263\n",
      "Epoch 8/10\n",
      "14/14 [==============================] - 6s 411ms/step - loss: 3275529.0000 - accuracy: 0.0266 - val_loss: 4651691.0000 - val_accuracy: 0.0263\n",
      "Epoch 9/10\n",
      "14/14 [==============================] - 6s 405ms/step - loss: 3275528.7500 - accuracy: 0.0266 - val_loss: 4651691.0000 - val_accuracy: 0.0263\n",
      "Epoch 10/10\n",
      "14/14 [==============================] - 6s 403ms/step - loss: 3275528.2500 - accuracy: 0.0266 - val_loss: 4651691.0000 - val_accuracy: 0.0263\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(input_tokens, output_tokens, \n",
    "                    epochs=10, \n",
    "                    batch_size=1024,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert RNN Results back to Translated Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(n, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == n:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict_sequence() missing 3 required positional arguments: 'model', 'tokenizer', and 'source'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-1a173012ec94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredict_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: predict_sequence() missing 3 required positional arguments: 'model', 'tokenizer', and 'source'"
     ]
    }
   ],
   "source": [
    "predict_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_sent2)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_to_word(RNN_result, tokenizer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# online model\n",
    "# def define_model(in_vocab, out_vocab, in_timesteps,out_timesteps, units):\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n",
    "#     model.add(LSTM(units))\n",
    "#     model.add(RepeatVector(out_timesteps))\n",
    "#     model.add(LSTM(units, return_sequences=True))\n",
    "#     model.add(Dense(out_vocab, activation='softmax'))\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = define_model(14662, 14662, 20, 20, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(pad_token1, pad_token2.reshape(pad_token2.shape[0], pad_token2.shape[1], 1),\n",
    "#                     epochs=30, batch_size=512, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
